{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Travel Time Distance Matrices, Access, and Coverage Metrics  \n",
    "\n",
    "<div style=\"text-align: center\"> \n",
    "[Irene Farah](https://www.linkedin.com/in/imfarah/), [Richard Lu](https://www.linkedin.com/in/richard-lu-576874155/), [Caitlyn Tien ](https://www.linkedin.com/in/caitlyn-tien-0b784b161/)  \n",
    "Center for Spatial Data Science  \n",
    "University of Chicago  \n",
    "July 30, 2019\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This notebook describes the reasoning of the Python modules p2p, ScoreModel, and CommunityAnalytics which compose the calculation of the algorithm of travel time **matrices** and the estimation of the **metrics**. Please note that this report stipulates the **_parameters_** specified within the estimations so the users can reproduce the calculations.\n",
    "\n",
    "The purpose of this report is threefold: present the comprehensive methodology for efficiently estimating travel time network distances, generating useful metrics, and illustrating its usage. Specifically, we create an open-source backend infrastructure for estimating travel time network distances (walking and driving), providing the framework for calculating **access** and **coverage** metrics. These two metrics address both the demand and supply of services and goods. On the one hand, the access score focuses on the attributes of the **origins** while on the other, the coverage score focuses on the attributes from the **destinations** of interest. Moreover, this framework allows subsetting of the data according to the user's specific questions of interest. These analyses create a powerful tool for supporting stakeholders and researchers in improving transparent decision-making processes and in understanding the issues of unequal spatial access.\n",
    "\n",
    "In this particular illustration, we analyze health facilities in the city of Chicago. The data is provided from http://makosak.github.io/chihealthaccess/index.html. However, this demo is provided so users can use their own data, by specifying the destinations and the spatial area of interest. \n",
    "\n",
    "This notebook provides the overview and logic of the methodology.  \n",
    "The contents of this folder lives in https://github.com/GeoDaCenter/access/travel_times.\n",
    "\n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:  \n",
    "More seasoned users can edit the Python scripts directly.  \n",
    "The package is written in Python 3.6, C++ 11, and Cython by [Logan Noel](https://www.linkedin.com/in/lmnoel/). (Minimum Python version 3.5)  \n",
    "Currently, the only supported operating systems are MacOS and Ubuntu (if you don't have either, a guide for installing Ubuntu 16.04 LTS is in README.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview  \n",
    "---\n",
    "Why is this method better?  \n",
    "\n",
    "This method comprehensively measures access **more efficiently** under an **open-source** and **scalable** framework that runs **offline** for confidentiality sake, compared to the extant state of the art options.  \n",
    "\n",
    "Generating large shortest path matrices for different transit types is an important tool for spatial data science, but does not currently have a solution that is open source, highly scalable and efficient. Several tools currently exist which fill a similar purpose to this software package. OSRM, Valhalla, Graphhopper and Google Maps, among other services, offer matrix APIs to compute the shortest path distance for datasets but the open-source solutions break down when applied to very large datasets. On the one hand, both Graphhopper and GoogleMaps charge for the service. On the other hand, creating a pgRouting, OSRM, and Valhalla solution does not properly scale. Lastly, OpenTripPlanner does scale, but it is not as fast as p2p.\n",
    "\n",
    "Each of the above services caps the number of entries in a request at 25-50, meaning that generating a matrix with 500,000 rows requires breaking the original matrix into millions of submatrices and making millions of individual queries. This approach works well for small datasets, but includes substantial overhead which is prohibitive on a large scale. The point to point shortest path algorithm (p2p) can generate matrices between a set of origin and destination points (or origins-origins) in 2 lines of code, efficiently and with a low memory footprint. \n",
    "\n",
    "This code generates a driving shortest path matrix for 46,251 blocks in Chicago in ~14 minutes (18 minutes for walking) whereas the same task took > 18 hours using MapZen's Valhalla. For this particular dataset, the mean difference between time values for the driving shortest path matrix and Google Maps' Matrix API is 2 minutes.\n",
    "\n",
    "**Matrices**  \n",
    "Specifically, there are two routes the user can take for getting the travel distances: Creating **asymmetric** or **symmetric** matrices. Symmetric matrices are estimated origin to origin, while asymmetric matrices calculate origin to destination. The user can generate a symmetric distance travel matrix and snap the points of interest to the matrix or create an asymmetric distance matrix that already incorporates origin and destination. The symmetric approach is more appropriate when you need to calculate several scores (across years or different types of scores) for the same area and at the same level of analysis.  \n",
    "\n",
    "**Metrics**  \n",
    "After obtaining the distance travel times from origins to destinations (in this case, from the centroids of tracts to the health facilities), the user can calculate the  **access score** from each tract areal unit to health facilities, the time to **closest destinations** (nearest neighbor), the **count of nearby destinations** within a buffer, the **sum of an attribute** of nearby destinations (wthin a community area), the destination's **coverage score** (per capita spending within a buffer), and the amount of per capita spending each tract areal unit has access to using a **Two Stage Floating Catchment Area** method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall infrastructure of the methodology is shown in the following figure:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/diagram_code.png\" width=\"1900\"  title=\"Optional title\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The workflow first estimates a point to point shortest path [**(p2p)**](./spatial_access/p2p.py) algorithm for creating the travel-time distance matrix by travel mode (walking and driving). Then, it creates a base model infrastructure [**(BaseModel)**](./spatial_access/BaseModel.py) for creating the metrics, using the ModelData class (parent of Access and Coverage Models created below). Specifically, this class allows the user to generate any type of metric, suiting each user's needs. Finally, it creates the models [**(Models)**](./spatial_access/Models.py)  for creating aggregate measures of the access score, AccessTime, AccessCount, AccessSum, DestinationSum, coverage score, and TSFCA output score (creating the Models classes). \n",
    "\n",
    "This framework provides the user with the flexibility to start at different stages along the process:  \n",
    "\n",
    "1) Start by creating an asymmetric travel time matrix using the p2p algorithm.  \n",
    "2) Start by creating a symmetric travel time matrix using the p2p algorithm and then subsetting it to create an asymmetric travel time matrix.  \n",
    "3) Already has a travel time matrix and wants to make use of the metrics.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The folder is organized as follows:**\n",
    "\n",
    "In the **docs** folder, under **notebooks**, the notebooks run through Chicago's health example:  \n",
    "* [reqs.ipynb](./docs/notebooks/reqs.ipynb)  : This notebook shows the installation and files requirements in order to run the demos.  \n",
    "* [matrix.ipynb](./docs/notebooks/matrix.ipynb)  : This notebook shows how to run the travel time distance matrices.  It uses the [p2p](./spatial_access/p2p.py) script.\n",
    "* [access_score.ipynb](./docs/notebooks/access_score.ipynb)  : This notebook shows how to run the access score and the specific parameters that might be tweaked depending on the user's interest.  It uses the [BaseModel](./spatial_access/Basemodel.py) and [Models](./spatial_access/Models.py) scripts.\n",
    "* [coverage_score.ipynb](./coverage_score.ipynb)  :This notebook shows how to run the coverage score and the specific parameters that might be tweaked depending on the user's interest.  It uses the [BaseModel](./spatial_access/Basemodel.py) and [Models](./spatial_access/Models.py) scripts.\n",
    "* [travel_time_metrics.ipynb](./docs/notebooks/travel_time_metrics.ipynb)  :This notebook shows how to run the travel time distance matrices. It uses the [BaseModel](./spatial_access/Basemodel.py) and [Models](./spatial_access/Models.py) scripts.  \n",
    "* [calibration.ipynb](./docs/notebooks/calibration.ipynb)  : **? Have it as a notebook/within the demos/in this notebook or not at all?** Comparison (validation and calibration) with GoogleMaps.\n",
    "\n",
    "In the **data** folder, the **input_data** folder contains the files needed for the estimation of the metrics under **sources** (for origins) and **destinations** (for destinations). In the **data** folder within **spatial_access**, the **matrices** folder contains the estimated symmetric and asymmetric matrices. The **access_score**, **coverage score**, **tsfca**, and **travel_time_metrics** folders contain the results of the analyses. Finally, **figures** contain the results of maps and plots calculated during the process. \n",
    "\n",
    "Further editing of the Python scripts is accessible under the folder **spatial_access**. \n",
    "\n",
    "The rest of this notebook describes the reasoning of the Python modules p2p, BaseModel, and Models which compose the calculation of the algorithm of travel time **matrices** and the estimation of the **metrics**. Please note that this report stipulates the **_parameters_** specified within the estimations so the users can reproduce the calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Matrices \n",
    "(Disregard this section if you already count with a travel-time matrix.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OpenStreetMap structure**  \n",
    "In order to elucidate how the algorithm for estimating the travel time distance, a brief description of OSM's structure follows.  \n",
    "OSM's data structure is composed of four elements: nodes, ways, relations, and tags.\n",
    "Nodes are latitude and longitude coordinates (projected in WGS 84) that represent the mapâ€™s features. Ways are a list of nodes that compose the geometry features (i.e. point, line, polygon) within a map, depicting streets, waterways, parks, etc. Relations express the relationship between nodes and ways. Lastly, tags are attached to nodes, mays or relations, storing metadata about the map objects.  \n",
    "\n",
    "Then, the OpenStreetMap network is downloaded using the area of the previously determined bounding box. The complexity of the network depends on the number of nodes on the generated bounding box (i.e. the area of interest defined by the latitude and longitude coordinates). Therefore, the number of observations should not affect the efficiency of the running times. In order to get the distances from OSM, OSM net calculates the distances of the relations, creating the edges that are queried for the travel time estimation. To estimate these distances, the user has to specify data into a WGS 84 projection. (Please find more specifics in the [requirements file](./docs/notebooks/reqs.ipynb).)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P2P (point to point) algorithm**  \n",
    "In order to calculate the network distance matrix, first, the code extracts the extreme value of latitude/longitude from the origin input table to create a bounding box of the area of interest. The size of the bounding box is increased by 'epsilon', to avoid cutting off the network of datapoints near the boundary of the bounding box. Please view the epsilon calibration to assess the appropriate value of epsilon for each estimation. **(PENDING or embedded in the demo?)**  \n",
    "\n",
    "P2P uses a k-d tree to match each point in the origin and destination data to its nearest neighbor node in the OSM network, and then finds the Vincenty distance between the two points. Vincenty's formulae estimate the geodesic distance between two points according to an ellipsoidal model of the Earth. (in this case are we using the direct or indirect formula?) Haversine formulas? more accurate? 0.3%>>>  \n",
    "\n",
    "For the travel time computation between origin and destination, Dijkstra's algorithm considers every possible route and chooses only the fastest route. Therefore, P2P also uses an adjacency list representation for Dijkstra's algorithm to find the shortest path for every node to every other node in the underlying OSM network, but it can skip doing any processing for nodes that do not have an attached origin data point. The advantage of this approach is that it scales to essentially any size dataset; as opposed to the adjacency matrix representation (which can easily exceed the memory of many systems for reasonably large datasets) P2P never loads the entire network into memory at one time, meaning the memory footprint is relatively small. This also means the multithreaded performance of P2P greatly outperforms the singlethreaded performance.  \n",
    "\n",
    "For every point in the origin dataset to every point in the destination dataset, the base impedence is the cost found using Dijkstra. To the base value we add the 'last mile' inferred impedence from the origin and destination points to their respective nearest nodes, determined by the Euclidean distance and a constant traversal speed. The 'last mile' is figurative; in the City of Chicago, for instance, 75 percent of block centroids were within 100 meters of the nearest OSM node and 95 percent of block centroids were within 200 meters.  \n",
    "\n",
    "**Islands**  \n",
    "Some of the units of analysis are classified as islands (disconnected nodes) with OSM. Therefore, Kosaraju's algorithm for directed graph strong connectedness is implemented in p2p (lines 713 - 805 of p2p.py under \\_request_network2 function). In graph theory, strong connectivity means that a path exists between any pair of nodes. Thus, we implement Kosaraju's algorithm to identify the disconnected nodes and we eliminate them from the network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Script**  \n",
    "The p2p.py script runs the point to point (p2p) algorithm and creates the class **TransitMatrix**. The output of p2p is the travel time matrix which is computed in seconds. The **TransitMatrix** unified class run manages all aspects of computing a transit time matrix where matrices can be symmetric or assymetric (as mentioned above). Therefore, load one input file if the user wants a symmetric distance graph, or two for an asymmetric matrix. Particularly, this class account for all the details that entail specifying the speed limits, creating the bounding box for the area of interest in order to run the OSM query and calculate the shortest path matrix.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Specifics of P2P parameters**  \n",
    "Several parameters should be taken into account when calculating the distance network matrix:  \n",
    "* The **network type** can be determined for walking, biking, or driving.  \n",
    "\n",
    "* Thresholds can be adjusted and are considered in the calculation of the distance matrix: the **average walking speed** is 5 km/h (3 mph) and the default **average driving speed** is of 40 km/h (25 mph).  This parameter might be specified differently for different populations.  * The City of Chicago estimates an average block dimension of 660 feet (200 m) by 330 feet (100 m).(https://www.cityofchicago.org/dam/city/depts/cdot/StreetandSitePlanDesignStandards407.pdf) STREET AND SITE PLAN DESIGN STANDARDS (2007:pg 2.) These dimensions might change across cities. >>> (Include table with conversions. Or reference to data on other cities) Therefore, the average walking speed of 3 mph estimates that a person, on average, walks a block in 72 to 144 seconds (1.2 - 2.4 min) to walk a block. The average speeds can be specified within the **p2p_parameters.json file**. For **driving** speed limits, open **speed_limit_dictionary.txt** file that contains speed limits for different OSM type of roads.    \n",
    "    \n",
    "* The **last mile** correction is the distance that is not accounted for when calculating the shortest path from the centroid of the block to the actual network; therefore, the wanting extra distance is added to the travel time distance(>>>). \n",
    "    \n",
    "* Also for walking and driving, a **node penalty** of X seconds can also be specified by the user for the number of intersections within the area of analysis. The logic is that having more intersections will increase the travel time due to crossings. However, by doing a time travel calibration between the p2p algorithm and GoogleMaps, there was no need for adding penalities for the city of Chicago. The calibration can be assessed for different cities by using a random set of latitude and longitude points and comparing both estimated travel times for p2p (using the default average speeds and speed limits) and the GoogleMaps API. <span style=\"color:magenta\"> **(PENDING)** <span style=\"color:black\"> (Show table in which the calibration was tested for different cities, suggesting values for each.) It can be specified within the **Configs.py** file.\n",
    "    \n",
    "* The network is **directed**, meaning that one way streets are respected and A->B and B->A can have different edge traversal speeds.  \n",
    "\n",
    "* **Threshold constants**: the walk or drive average speed can be specified within the **Configs.py** file.\n",
    "\n",
    "* **Epsilon**: Controls how large to make the network bounding box beyond your dataset. Larger epsilons result in longer computation times, but smaller epsilons result in slightly reduced accuracy at the very edges of the bounding box, especially for driving networks. The default is currently set at 0.05, which seems to balance the two reasonably well. (+/-) 0.02 will result in a large increase/decrease in computation time and accuracy. If too many values are defined as -1, it means that the epsilon is too small. Refer to the epsilon calibration to assess if this value must change and the matrix contains too many -1. The value of -1 is hardcoded in the tmat.h file and is considered as an NaN value of the origins when estimating the metrics.  \n",
    "\n",
    "* Please note that peak hour is not considered in the estimation of travel time distances. However, according to the GoogleMaps calibration, the travel time conditions we are asumming are in between rush hour and no traffic. <span style=\"color:magenta\"> **(PENDING)**  \n",
    "\n",
    "Please look at the [requirements file](./reqs.ipynb) to comply with the files requirements. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Specifications_**  \n",
    "\n",
    "In the **demo prompt**, the user can specify: **network type, primary input data (origin file), secondary input data (destination file if running an asymmetric matrix), and if the user wants to save the matrix to a csv file.**  \n",
    "\n",
    "In the **Configs**, the user can specify:  **Average speeds, last mile corrections, node penalty, epsilon, threshold constants, and output results** (meters instead of seconds - lines 580 - 599 of p2p.py).  \n",
    "\n",
    "In order to construct the matrices, the input table should contain **ID, latitude, longitude** for the origins and destinations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='communityanalytics'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Metrics\n",
    "(Disregard this section if you only care about the travel-time matrix.)\n",
    "\n",
    "**Scripts**  \n",
    "Models.py creates the classes: **AccessModel** and **Coverage**. As mentioned above, the access score, time to closest destination, sum of nearby attributes, TSFCA (access to per capita spending), and count of nearby destinations metrics are attributes of the origins while the coverage score is an attribute of the destinations in a catchment area. The access to faciilities, acccess to per capita spending, coverage, and count of nearby destionation metrics are all estimated within a travel time network distance buffer. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Origin-based**  \n",
    "\n",
    "**Access Model**  \n",
    "Across disciplines, accessibility indicators provide a useful measure for a myriad of questions, like analyzing who does and does not live within reach of specific amenities/services or for estimating the spatial mismatch between supply and demand of these services. Thus, we generate an **Access** score that indicates the proximity to amenities. The score considers the centroids of the areas of analysis as the origin and the amenities as the destination points. It estimates the score by considering a distance decay function, the density and the variety of categories within each area of analysis.    \n",
    "\n",
    "Depending on the user's preference, the categories can be chosen from the pool of datapoints by creating a dictionary (example shown below). The dictionary will contain the weight that each category will have in the estimation of the score. In this case, a hospital will be categorized as more important than a smaller clinic. However, the weights can be specified according to each user's needs. Moreover, the dictionary categorizes the second nearest clinic as having less weight than the first one. The user may not care for having a greater number of counts per category. The density weights will create a larger score for areas that have a greater variety of categories, creating a more diverse environment. The variety of categories penalizes areas that have the same type of category within an area. Nevertheless, in some cases (like the Chicago health facilities), the repetition of categories should not be penalized because the demand for those services (hospitals) is higher than the service's provision capacity. Therefore, in this example we categorize as equally important the nearest hospital as the fifth nearest hospital. If there is a sixth hospital within a 30 minute buffer of a tract, the score will neglect it since there are only 5 weights specified under 'Hospitals'. As we mentioned before, in this particular case, the dictionary will only take into account the destinations (health facilities) that are within a 30 minute network distance buffer. Therefore, destinations further than 30 minutes from the source (in this case, can be specified in 'upper') will be neglected in the calculation.\n",
    "As a result, the number of establishments/facilities within a 30 minutes buffer will be aggregated at the areal unit of analysis providing a total count of weights.\n",
    "\n",
    "**Note**: Please be aware of any mispelling in defining the weights. The score will neglect any category that is mispelled in the definition of the weights.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of density and variety weights:\n",
    "dict = {\n",
    "\"Hospitals\": [10,10,10,10,10],\n",
    "\"Federally Qualified Health Centers\": [8, 7, 6, 5, 4],\n",
    "\"School-Based Health Centers\": [7, 7, 6, 6, 5],\n",
    "\"All Free Health Clinics\": [5, 5, 5, 4, 4],\n",
    "\"Other Health Providers\": [4,3,2,1,1]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the distance decay function describes the decreasing intensity of a value as the distance increases. This notebook provides three functions shown below: linear, square root, and logit. However, the user can add any function depending on the amenity's intensity behavior.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance decay functions shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/dd.png\" width=\"1400\" title=\"Optional title\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate the access score, each count weight is multiplied by the distance decay function. The distance decay function will weigh each count depending on its relative distance to the source. Therefore, closer destinations will be weighed higher that further away destinations. The figure below whos how the establishments within a buffer are weighted according to the weights dictionary and then weighted by the distance decay function. The image shows how a 10 minute travel time to a hospital is  weighted by the distance decay function, giving it a final score of 9 which is then aggregated to the other scores within a 30 minute buffer. The closest Federally Qualified Health Center is weighted as 8 and the second closest health center is weighted at 7, both multiplied by their corresponding distance decay. Finally, each score is aggregated to generate the final score per area of analysis. In the output, the score is purposely not normalized in order to observe the overall distribution across cities and years, but the standarized and unstandarized results are also included in the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/flow.png\" width=\"1400\"  title=\"Optional title\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Specifications_**  \n",
    "\n",
    "In the **demo prompt**, the user can specify parameters in two different commands:  \n",
    "\n",
    "**name.AccessModel( ):**  \n",
    "- network_type ('walk', 'bike', 'drive', 'otp') \n",
    "- sources_filename (primary input data)\n",
    "- destinations_filename (secondary input data)\n",
    "- source_column_names (dictionary that map column names to expected values)\n",
    "- dest_column_names (dictionary that map column names to expected values)\n",
    "- transit_matrix_filename (sources-destination travel time matrix)\n",
    "- decay_function ('linear', 'root', 'logit', default is 'linear')\n",
    "**name.calculate():**\n",
    "- upper_threshold (maximum time considered out of range in seconds, default is 30 minutes, when out of range, score will be zero)\n",
    "- category_weight_dict (specifies the weights of each facility/establishment defined as dictionary, default dictionary will contain [1,1,1,1,1,1,1,1,1,1] weights.)\n",
    "- normalize (accepts boolean, default is False and shows only non-normalized results, true shows normalized values.)\n",
    "- normalize_type ('z_score' or 'minmax', default is 'minmax')\n",
    "\n",
    "In the **Models**, the user can specify:  The default weights set before as [1,1,1,1,1,1,1,1,1,1] weights, different standarizations, and other modifications to the access score.\n",
    "\n",
    "GO TO [ACCESS SCORE DEMO](./access_score.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Travel time metrics**  \n",
    "The travel time metrics contains two other origin based metrics: the time to closest destination and the count of nearby destinations.\n",
    "\n",
    "These two metrics are focused on the destination's categories. Therefore, the results are shown for each specific type of amenity.  \n",
    "On the one hand, the time to closest destination shows the seconds it takes from each area of analysis (tract) to the closest amenity. This metric disregards the 30 minute buffer.\n",
    "On the other hand, the count of nearby destinations aggregates the number of amenities of each category within a buffer.\n",
    "**_Specifications_**  \n",
    "\n",
    "In the **demo prompt**, the user can specify parameters in two different commands:  \n",
    "\n",
    "**name.TTMetrics( ):**  \n",
    "- network_type ('drive' or 'walk')\n",
    "- source_filename (primary input data) \n",
    "- dest_filename (secondary input data)  \n",
    "- sp_matrix_filename (origin-destination travel time matrix)    \n",
    "- limit_categories (only estimate models for specific categories)\n",
    "\n",
    "**name.calculate():**\n",
    "- Just processes the model specified above. \n",
    "\n",
    "\n",
    "In the **scripts**, the user can specify:  More specific designed metrics if needed.\n",
    "\n",
    "GO TO [TRAVEL TIME METRICS DEMO](./travel_time_metrics.ipynb) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Destination-based**  \n",
    "The **Coverage** model generates a coverage access which shows the per capita spending available to a specific targeted population. The model focuses on the coverage of the destination, scrutinizing how many people are within a catchment area. Specifically, it takes the total spending of the facility/establishment and divides it by the total population it serves within a buffer (in this case, 30 minutes). In the specifications, the magnitude of the destination is denominated as target.  \n",
    "\n",
    "**_Specifications_**  \n",
    "\n",
    "**name.Coverage( ):**  \n",
    "- network_type ('walk', 'bike', 'drive', or 'otp')\n",
    "- sources_filename (primary input data)\n",
    "- destinations_filename (secondary input data)\n",
    "- transit_matrix_filename (origin-destination transit matrix)\n",
    "        \n",
    "**name.calculate():**\n",
    "- upper_threshold (the time (in seconds) in which the origin and destinations are considered to be out of range of each other)\n",
    "\n",
    "In the **Models**, the user can specify further changed to the coverage score.\n",
    "\n",
    "GO TO [COVERAGE SCORE DEMO](./coverage_score.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/coverage.png\" width=\"600\" title=\"Optional title\"  align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final notes**  \n",
    "The scores can be aggregated at a greater areal unit, as is shown in the demos. The aggregation of the access score is by multiplying all the values at the finer scale, while the aggregation of the coverage score is the sum of the smaller units of analysis. The scripts also contain hard coded empirical cumulative distribution function plots that are also exemplified in the notebooks. The output of the scores is in csv files and can be merged into the origin's or destination's shapefile, mapping out the results of the scores for further analysis.  \n",
    "\n",
    "The field names in the csv output that contain any symbols will be replaced as underscores in the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical notes  \n",
    "\n",
    "#### Installing a fresh copy of Ubuntu 16.04 LTS with pip3\n",
    "---\n",
    "\n",
    "1. Follow the instructions at this link: https://linus.nci.nih.gov/bdge/installUbuntu.html\n",
    "2. Run `sudo apt-get update`\n",
    "3. Run`sudo add-apt-repository universe`\n",
    "4. Run`sudo apt-get -y install python3-pip`\n",
    "\n",
    "## Generate a virtual machine >>>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
